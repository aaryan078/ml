{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and Word Vectors\n",
    "\n",
    "- Two thoughts of Prof Christopher Manning  about language:\n",
    "    - Language is such an evolved system of communication, but very **uncertain**.\n",
    "        - However humans have some agreed meaning which helps us communicate so well.\n",
    "        - We are internally and subconsciously doing some kind of probabilistic inference to determine meaning not just for information but also for social functions etc.\n",
    "    - For artificial intelligence to reach at a very sophisticated level, it needs to be able to capture all of the human knowledge, which is predominantly conveyed through human language. \n",
    "        - Human language is our networking language through which we collectively form a huge network of individuals.\n",
    "        - Human language made human being invincible. Language made humans to be able to work collectively as a group or team.  That is how they evolved not to just survive in a world of more powerful animals but they thrived.\n",
    "        - Invention of writing made this knowledge to shared spatially (i.e. through space) or temporally (i.e. through time) not just verbally. \n",
    "        - Writing is very recent (~5000 years) phenomenon in scale of evolution, but made humans super powerful.\n",
    "        - We compress knowledge efficiently and provide a view of the world in very few bits of information (e.g. I went to Zoo and saw an elephant. When you read this it constructs the whole visual scenery in your mind with images which can take few megabytes to store in a computer, but was communicated in very little words).\n",
    "\n",
    "### How do we represent the meaning of the words?\n",
    "- Linguists use something called Denotational Semantics to think about meaning.\n",
    "    - Linguists think of meaning as what things represent.\n",
    "    - $$\\text{signifier(symbol)}  \\leftrightarrow \\text{signified{(idea or thing)}}$$\n",
    "    - Word \"chair\" representing all the thing that are chair.\n",
    "    - Word \"running\" represents a set of actions people do, which represents the activity those actions perform i.e. 🏃‍♂️ \"\n",
    "\n",
    "### How do we have reasonable meaning in a computer?\n",
    "- Common Solution: Something like `WordNet` which is a thesaurus containing words and their relationships using  synonym set and hypernyms (\"is a\" relationship)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ramand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Synonyms of \"good\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun : good\n",
      "noun : good, goodness\n",
      "noun : good, goodness\n",
      "noun : commodity, trade_good, good\n",
      "adjective : good\n",
      "adjective(s) : full, good\n",
      "adjective : good\n",
      "adjective(s) : estimable, good, honorable, respectable\n",
      "adjective(s) : beneficial, good\n",
      "adjective(s) : good\n",
      "adjective(s) : good, just, upright\n",
      "adjective(s) : adept, expert, good, practiced, proficient, skillful, skilful\n",
      "adjective(s) : good\n",
      "adjective(s) : dear, good, near\n",
      "adjective(s) : dependable, good, safe, secure\n",
      "adjective(s) : good, right, ripe\n",
      "adjective(s) : good, well\n",
      "adjective(s) : effective, good, in_effect, in_force\n",
      "adjective(s) : good\n",
      "adjective(s) : good, serious\n",
      "adjective(s) : good, sound\n",
      "adjective(s) : good, salutary\n",
      "adjective(s) : good, honest\n",
      "adjective(s) : good, undecomposed, unspoiled, unspoilt\n",
      "adjective(s) : good\n",
      "adverb : well, good\n",
      "adverb : thoroughly, soundly, good\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "poses = {'n' : 'noun', 'v' : 'verb', 'a' : 'adjective', 's' : 'adjective(s)', 'r' : 'adverb' }\n",
    "for synset in wn.synsets(\"good\"):\n",
    "    print(f'{poses[synset.pos()]} : {\", \".join([l.name() for l in synset.lemmas()])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hypernyms of \"Tiger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('person.n.01'),\n",
       " Synset('causal_agent.n.01'),\n",
       " Synset('organism.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('living_thing.n.01'),\n",
       " Synset('entity.n.01'),\n",
       " Synset('whole.n.02'),\n",
       " Synset('object.n.01')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiger = wn.synset(\"Tiger.n.01\")\n",
    "hyper = lambda t: t.hypernyms()\n",
    "list(tiger.closure(hyper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- WordNet is outlining various use of \"good\" in English. They are very fine grained difference which humans can barely understand.\n",
    "- It clearly misses nuance. e.g. expert is not really \"good\"\n",
    "- It also misses new meanings of word e.g. 'wicked good'. It is based on human labor and impossible to keep up-to-date.\n",
    "- It also can't give us accurate word similarity and a score of how similar a pair of words are?\n",
    "- It is very subjective.\n",
    "\n",
    "\n",
    "### Localist Distribution\n",
    "This is tangential to current discussion.\n",
    "- A representation of space or collection where each entity is represented independently in a space.\n",
    "- It can only describe a number of distinct object that are linear in number of dimension.\n",
    "- This representation do not represent any relationship between the entities.\n",
    "- One Hot Encoding is a localist representaiton.\n",
    "- If we represent each word as a symbol, English language is estimated to have 13 million words. If we represent each word as a vector of 13 million dimension with one 1 and rest 0.\n",
    "- In Neurology, localist representation theorizes that each neuron is a single concept on a stand alone basis. Each neuron or localist unit which has \"meaning and representation\"\n",
    "- This is inverse of Distributed Representation.\n",
    "\n",
    "\n",
    "### Representing words as discrete symbols.\n",
    "- **Pre-2012**\n",
    "    - Words as discrete symbols in lexicon (vocabulary)\n",
    "    - \"hotel, conference, motel\" a localist representation\n",
    "    - One Hot Encoding is used to represent the words as vector\n",
    "        - motel = [00000100]\n",
    "        - hotel =  [00000010]\n",
    "    - These vectors become huge because languages have lot of words.\n",
    "    - In language like English, we can have almost infinite words by using Derivation Morphology \n",
    "        - \"New words are created in language by adding more words to the ending of existing words.\"\n",
    "        - \"e.g. Paternal --> Paternalistic, Paternalistically \"\n",
    "        - \"This can explode the vocabulary of a language by many folds.\"\n",
    "    - This takes huge computational power as word vector can be of dimension 500,000 or more.\n",
    "    - Another bigger problem is often times we are interested in relationship and meaning of words.\n",
    "        - If I search for \"Seattle Motels\", I might also actually like \"Seattle Hotels\" in the search too.\n",
    "        - However,  words as a symbol representation keeps these words orthogonal in the space. See One Hot Encoding above. \n",
    "        - There is no notion of similarity between one-hot encoded vectors.\n",
    "        - Word Similarity tables can solve this problem but that means but that explodes the computational problem. For each pair of words you keep a score of how similar they are but leads to really large table e.g.  using 500,000 words in vocabulary we might end up with a table of 2.5 trillion cells.\n",
    "- Instead of that how about **we encode \"similarity\" in vector themselves?**\n",
    "\n",
    "### Distributional Semantics\n",
    "- Linguistic meaning of the word: A word's meaning is given by the words which appear close to this word frequently. \n",
    "- When a word *w* appears in the text, it's **context** is the set of words that appear near *w*  at a fixed length window.\n",
    "- \"You shall know a word by the company it keeps\" - J. R. Firth 1957\n",
    "- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fdailygrind%2FlUwgP_m7kQ.png?alt=media&token=933154b6-2990-4d0a-aa17-323b7ddabf13)\n",
    "- The meaning of \"banking\" is the collections of all the words around it.\n",
    "\n",
    "\n",
    "### Distributed Representation\n",
    "\n",
    "- This idea is inverse of localist representation e.g. One Hot Encoding.\n",
    "- In One Hot Encoding each vector is independent of the other words and represented as really large vectors (English has 13 million different words) where each vector has all 0s but one 1 which represent that word.\n",
    "    - motel = \\[00000100...00\\]\n",
    "    - hotel  = \\[00000010...00\\]\n",
    "- In distributed representation, each word is represented as a dense vector which is similar to vector of words that appear in similar contexts.\n",
    "- In other words, words which appear together live closer in the vector space representing all the words. e.g. motel and hotel will be related words and will live close in the vector space.\n",
    "- The dimensions of this vector is very small compared to the vocabulary e.g. 50, 100, 200.... 4000 as compared to 13 million English words. \n",
    "- We use this smaller vector space to encode the relationship between words.\n",
    "$$ \\text{banking} = \\begin{pmatrix} 0.102 \\\\ 0.432 \\\\0.445\\\\ 0.001\\\\0.034 \\end{pmatrix}$$\n",
    "- These word vectors are sometimes called Word Embeddings or word representations.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
