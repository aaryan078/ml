{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing word2vec from scratch\n",
    "\n",
    "In this notebook we will implement word2vec (skipgram method) from scratch using numpy. To see detailed notes, go to lecture 1 notebook of CS224N - [Lecture-1-Introduction-and-Word-Vectors](Lecture-1-Introduction-and-Word-Vectors.ipynb). Here we jump directly to code.\n",
    "\n",
    "As a first step, let's first generate the training data we are going to use train the word2vec.\n",
    "\n",
    "## Variables\n",
    "\n",
    "We will use the following variables in our implementation of `word2vec`.\n",
    "\n",
    "- `V` : Number of unique words in the corpus (vocabulary)\n",
    "- `x` : Input layer (One hot encoding of our input word)\n",
    "- `N` : Number of neurons in the hidden layer of neural network i.e. the dimension of the vector representation of the word.\n",
    "- `W` : Weights between input layer and hidden layer.\n",
    "- `W'`: Weights between hidden layer and output layer.\n",
    "- `y` : A softmax layer having probabilities of each word in our corpus\n",
    "\n",
    "![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fdailygrind%2FiX7rd0bVNJ.png?alt=media&token=5ac14c23-1165-4a20-be6c-c1dac015d50d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation details\n",
    "\n",
    "- Input Layer (`x`): It is one hot encoding of the current center word. It's size of is *V* x 1, where *V* is the size of the vocabulary/unique words in corpus.\n",
    "- Weight Matrix (`W`) : It is the matrix of the representation of the all center words in the vocabulary. There will be *V* rows (one for each word in vocabulary) and each row vector has N dimension, hence its size is V x N.\n",
    "- Hidden Layer (`h`): It is nothing but representation of the center word in column vector. It is calculated by \n",
    "    $$\n",
    "        h = W^Tx\n",
    "    $$\n",
    "- Weight Matrix 2 (`W'`): It is the matrix which stores the context word representation of each word in vocabulary. However, here each word is represented as column. There will be *V* columns of *N* dimension for each word in vocabulary. Thus shape will be N x V.\n",
    "\n",
    "- Output Layer (`y`): This is where we calculate similarity between center word with the chosen context word, and then calculate softmax of the similarity (nothing but cosine similarity). The softmax converts the integer values to probability distribution.\n",
    "\n",
    "- Forward propagation: The process of moving getting the output layer `y` from `x` is forward propagation.\n",
    "- Backward propagation: In this process, based on the loss we calculate using gradient descent, we propagate the loss values in backward direction and update the `W` and `W'` in the process.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Generating training data.\n",
    "\n",
    "As described in the lecture notebook, the idea is to go through the corpus and treat each token as center word, and tokens around it as context words. We are not going to use fancy methods of tokenization here, but using those, the word2vec can perform amazing. We also maintain the two dictionaries to map word to index and index to word (This is the how we represent o as index of outside word and c as index of center word and map it to j in the index formula) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def generate_training_data(corpus, context_size):\n",
    "    word_counts = defaultdict(int);\n",
    "    tokens = []\n",
    "    X, y = [], []\n",
    "    for sentence in corpus:\n",
    "        word_list = sentence.split()\n",
    "        for word in word_list:\n",
    "            word = word.lower()\n",
    "            word_counts[word] += 1\n",
    "            tokens.append(word)\n",
    "    N = len(word_counts.keys())\n",
    "    words_list = sorted(list(word_counts.keys()), reverse=False)\n",
    "    word_index = dict((word, i) for (i, word) in enumerate(words_list))\n",
    "    index_word = dict((i, word) for (i, word) in enumerate(words_list))\n",
    "    \n",
    "    for i in range(N):\n",
    "        context_indices = list(range(max(0, i - context_size), i)) + \\\n",
    "            list(range(i+1, min(i + context_size + 1, N)))\n",
    "        for j in context_indices:\n",
    "            X.append(word_index[tokens[i]])\n",
    "            y.append(word_index[tokens[j]])\n",
    "        \n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    y = np.array(y)\n",
    "    y = np.expand_dims(y, axis=0)\n",
    "    \n",
    "    return (X,y, word_index, index_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just briefly test this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "Vocabulary size: 38\n",
      "Shape of training data:  (1, 216) (1, 216)\n",
      "Word to index:  {'a': 0, 'after': 1, 'and': 2, 'banana': 3, 'bananas': 4, 'been': 5, 'buckets.': 6, 'by': 7, 'carrying': 8, 'convince': 9, 'could': 10, 'death': 11, 'doing': 12, 'else': 13, 'everyone': 14, 'few': 15, 'fields': 16, 'give': 17, 'have': 18, 'heaven': 19, 'him': 20, 'history': 21, 'in': 22, 'is': 23, 'limitless': 24, 'monkey': 25, 'never': 26, 'people': 27, 'ploughing': 28, 'promising': 29, 'something': 30, 'that': 31, 'to': 32, 'very': 33, 'was': 34, 'water': 35, 'while': 36, 'you': 37}\n"
     ]
    }
   ],
   "source": [
    "corpus = ['''You could never convince a monkey to give you a banana by promising him limitless bananas after death in monkey heaven''',\n",
    "'''History is something that very few people have been doing while everyone else was ploughing fields and carrying water buckets.''']\n",
    "(X, y, word_index, index_word) = generate_training_data(corpus, context_size=3)\n",
    "vocab_size = len(index_word)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "print(\"Shape of training data: \", X.shape, y.shape)\n",
    "print(\"Word to index: \", word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need each word to be represented as one-hot encoding representation. So let's do that too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2OneHot(word, word_index):\n",
    "    vocab_size = len(word_index)\n",
    "    y_one_hot = np.zeros((vocab_size))\n",
    "    y_one_hot[word_index[word]] = 1\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2OneHot('monkey', word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
