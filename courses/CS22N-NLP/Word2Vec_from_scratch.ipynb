{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing word2vec from scratch\n",
    "\n",
    "In this notebook we will implement word2vec (skipgram method) from scratch using numpy. To see detailed notes, go to lecture 1 notebook of CS224N - [Lecture-1-Introduction-and-Word-Vectors](Lecture-1-Introduction-and-Word-Vectors.ipynb). Here we jump directly to code.\n",
    "\n",
    "As a first step, let's first generate the training data we are going to use train the word2vec.\n",
    "\n",
    "## Generating training data.\n",
    "\n",
    "As described in the lecture notebook, the idea is to go through the corpus and treat each token as center word, and tokens around it as context words. We are not going to use fancy methods of tokenization here, but using those, the word2vec can perform amazing. We also maintain the two dictionaries to map word to index and index to word (This is the how we represent o as index of outside word and c as index of center word and map it to j in the index formula) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def generate_training_data(corpus, context_size):\n",
    "    word_counts = defaultdict(int);\n",
    "    tokens = []\n",
    "    X, y = [], []\n",
    "    for sentence in corpus:\n",
    "        word_list = sentence.split()\n",
    "        for word in word_list:\n",
    "            word = word.lower()\n",
    "            word_counts[word] += 1\n",
    "            tokens.append(word)\n",
    "    N = len(word_counts.keys())\n",
    "    words_list = sorted(list(word_counts.keys()), reverse=False)\n",
    "    word_index = dict((word, i) for (i, word) in enumerate(words_list))\n",
    "    index_word = dict((i, word) for (i, word) in enumerate(words_list))\n",
    "    \n",
    "    for i in range(N):\n",
    "        context_indices = list(range(max(0, i - context_size), i)) + \\\n",
    "            list(range(i+1, min(i + context_size + 1, N)))\n",
    "        for j in context_indices:\n",
    "            X.append(word_index[tokens[i]])\n",
    "            y.append(word_index[tokens[j]])\n",
    "        \n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    y = np.array(y)\n",
    "    y = np.expand_dims(y, axis=0)\n",
    "    \n",
    "    return (X,y, word_index, index_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just briefly test this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "Vocabulary size: 38\n",
      "Shape of training data:  (1, 216) (1, 216)\n",
      "Word to index:  {'a': 0, 'after': 1, 'and': 2, 'banana': 3, 'bananas': 4, 'been': 5, 'buckets.': 6, 'by': 7, 'carrying': 8, 'convince': 9, 'could': 10, 'death': 11, 'doing': 12, 'else': 13, 'everyone': 14, 'few': 15, 'fields': 16, 'give': 17, 'have': 18, 'heaven': 19, 'him': 20, 'history': 21, 'in': 22, 'is': 23, 'limitless': 24, 'monkey': 25, 'never': 26, 'people': 27, 'ploughing': 28, 'promising': 29, 'something': 30, 'that': 31, 'to': 32, 'very': 33, 'was': 34, 'water': 35, 'while': 36, 'you': 37}\n"
     ]
    }
   ],
   "source": [
    "corpus = ['''You could never convince a monkey to give you a banana by promising him limitless bananas after death in monkey heaven''',\n",
    "'''History is something that very few people have been doing while everyone else was ploughing fields and carrying water buckets.''']\n",
    "(X, y, word_index, index_word) = generate_training_data(corpus, context_size=3)\n",
    "vocab_size = len(index_word)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "print(\"Shape of training data: \", X.shape, y.shape)\n",
    "print(\"Word to index: \", word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need each word to be represented as one-hot encoding representation. So let's do that too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2OneHot(word, word_index):\n",
    "    vocab_size = len(word_index)\n",
    "    y_one_hot = np.zeros((vocab_size))\n",
    "    y_one_hot[word_index[word]] = 1\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2OneHot('monkey', word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
