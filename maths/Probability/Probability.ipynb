{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Probability and Random Processes\n",
    " ### Random Experiment\n",
    " A random experiment is a process by which we observe something uncertain. After the experiment the result of the experiment is known.\n",
    "   #### Trial\n",
    "   If a random experiment is repeated several times, we call each of them as trial. Thus a trial is a particular performance of a random experiment. In the example of tossing a coin, each trial will result in either heads or tails.\n",
    "   #### Outcome:\n",
    "   An outcome is the result of a  random experiment.\n",
    "   \n",
    "   #### Sample Space: \n",
    "   The set of all possible outcomes of a  random experiment is called sample space.\n",
    "\t\n",
    "   #### Event\n",
    "   An event **A** is subset of sample space **S** and we way that **A** __occurred__ if the actual outcome is in  **A**\n",
    "* example:: Before tossing a coin, you won't know whether you will get heads or tails. This is a random experiment. It's sample space is {__heads, tails__} usually denoted as {__H, T__}.  Another example is if toss a coin 3 times and observe the sequence of heads/tails. The sample space here may be defined as :S = {__(H, H, H), (H, H, T), (H, T, H), (T, H, H), (H, T, T), (T, H, T), (T, T, H), (T, T, T)__}. Let **A** can be an event that first flip is Heads.  It is indeed a subset of sample space.  A = {__(H, H, H), (H, H, T), (H, T, H)__, __(H, T, T)__}. saying that **A** occurs is the same thing as saying that the first flip is Heads.\n",
    "\n",
    "### Probability\n",
    "  A mathematical language for expressing degree of belief or uncertainties about events.\n",
    "  * *frequentist view*:: of probability is that it represents a long-run frequency over a large number of [Trial](Trial.md)s of a  random experiment. If we say a coin has 1/2 probability of heads, that means coin would land heads ~50% of time if we tossed it over and over and over.\n",
    "  * *bayesian view* :: Probability represents a degree of belief about the event in question. So that we can assign the probability to hypotheses like \"candidate A will win election\" or \"the defendant is not guilty\", even if it isn't possible to repeat the same election or crime over and over again.\n",
    "  \n",
    "   * *naive definition* :: The naive definition of probability of an event is to count the number of ways the event could happen and divide by the total number of possible outcomes for the experiment.\n",
    "   $$P_{naive}(A) = \\frac{|A|}{|S|} = \\frac{\\text{number of outcomes favorable to A}}{\\text{total number of outcomes in S}} $$\n",
    "   $$P_{naive}(A) = \\frac{4}{9} $$As there are 4 favorable outcomes to event A, and total number of outcomes in sample space is 9.\n",
    "\t\t\t\n",
    "   * *why naive* :: \n",
    "\t\t* Assumed that sample space is finite, and each outcomeis equally likely.\n",
    "\t\t* For example, Imagine two arguments: People think that either there is life on mars or there isn't. So they apply a 50:50 ratio without reasoning and make a conclusion that probability of life on mars is 1/2.  But by same logic probability of __intelligent__ life on mars is also 1/2. However, it is intuitively clear that latter should have strictly lower probability than former.\n",
    "\t* *where naive definition is applicable* :: \n",
    "        * Where there is __symmetry__ in problem that makes it equally likely. For example it is common to assume that coin is fair and there is equally likely to get a heads or tail due to physical symmetry of the coin. Unless mentioned, there is no doubt that a dice is fair and probability of getting any number from 1 to 6 is 1/6.\n",
    "\t\t* When the outcomes are equally likely by design. For example while conducting a survey of __n__ people in a population of __N__. A common goal is to obtain a simple random sample, which means that the n people are chosen randomly with all subsets of size n being equally likely.\n",
    "\t\t* When the naive definition serves a __null model__. In this case, we assume that naive definition applies and see what prediction we get and then compare them with real world observed data to asses if the hypothesis of equally likely outcomes is tenable.\n",
    "        \n",
    "Let's write some python code to use this definition. One quick think before we proceed is \n",
    "\n",
    "#### Bernoulli Trial\n",
    "A random experiment is called Bernoulli trial, after Jacon Bernoulli, when the possible outcomes are binary; they can be modelled as success or failure, yes or no, on or off. For example: a coin flip. Say success is when you get a heads and tails otherwise, or we can do vice versa.\n",
    "\n",
    "In the code below we simulate this coin flip as Bernoulli trial using `scipy.stats`. This code generate random variates. Argument *P* represent the probability of success and size represents number of trials (in this case, number of coin flips)\n",
    "P is 0.5 assuming coin is fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import bernoulli\n",
    "bernoulli.rvs(p=0.5, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many heads? (Assuming heads is success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(bernoulli.rvs(p=0.5, size=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sequence of independent Bernoulli trials follows the **Binary Distribution**. So instead of summing the outcomes using the `bernoulli` object, we can use *Binomial Distribution* using the `binom` object. The code has three argument *n* again is the number of trials (e.g. number of coin flips), *p* is the probability of success, and *size* is the number of draws of the same experiment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import binom\n",
    "\n",
    "binom.rvs(n=10, p=0.5, size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically we are saying we got 7 heads out of 10 flips (10 trials), when experiment was performed once. Let's try the same experiments 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 8, 3, 5, 3, 6, 4, 6, 4, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binom.rvs(n=10, p=0.5, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if coin is not fair and biased towards tails, so we are more likely to get tails by say probability of 0.7, in that case, we will have probability of heads as 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 3, 5, 2, 5, 3, 0, 5, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binom.rvs(n=10, p=0.3, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason the random modules are called pseudo-random because we can generate the same set of sequence by setting a random seed. The sequence of random numbers we get from a seed will always be same. We can set the random seed either by using `random_state` parameter like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 5, 4, 3, 2, 2, 1, 5, 3, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binom.rvs(n=10, p=0.3, size=10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or we can use the `np.random.seed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 5, 4, 3, 2, 2, 1, 5, 3, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "binom.rvs(n=10, p=0.3, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that both sequences are same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Variable\n",
    "\n",
    "A random variable is a real-valued variable whose value is determined by an underlying random experiment.\n",
    "Let's consider an example: I toss a coin 5 times. This is a random experiment, and the sample space of the experiment will be :\n",
    "\n",
    "$$\n",
    "\\text{S} = \\{\\textit{HHHHH}, \\textit{HHHHT}, ....., \\textit{TTTTT}\\}\n",
    "$$\n",
    "\n",
    "The sample space will have $2^{5} = 32$ elements. Suppose in this experiment, we are interested in number of heads. This can be done by defining a random variable ***X*** whose value is the number of heads observed. The value of ***X*** will be 0, 1, 2, 3, 4 or 5, depending on the outcome of the experiment.\n",
    "\n",
    "To put it in other words based on the above example, a random variable is a real-valued function that assigns a numerical value to each possible outcome of the random experiment. For instance, the random variable ***X*** defined above assigns following values to the outcomes.\n",
    "\n",
    "| Outcome |     X     |\n",
    "| :------------: | :----------: |\n",
    "| HHHHH| 5 |\n",
    "| HHHHT| 4 |\n",
    "| ..   | ..|\n",
    "| TTTTH| 1 |\n",
    "| TTTTT| 5 |\n",
    "\n",
    "Thus,  a random variable X is a function from the sample space to the real numbers.\n",
    "$$\n",
    "    X: S \\rightarrow \\mathbb{R}\n",
    "$$\n",
    "\n",
    "Random variables are usually denoted by capital letters such as ***X***, ***Y*** and  ***Z***. Since the *Random Variables* are actually functions, it's range can be defined. The range of random variable ***X*** shown by Range(***X***) or $\\textit{R_{x}}$ is the set of all possible values of ***X***. In the above example, $Range(X) = R_{x} = \\{0, 1, 2, 3, 4, 5\\}$\n",
    "\n",
    "#### Discrete Random Variable\n",
    "\n",
    "Discrete Random Variable are random variables whose range is countable set. The above example is clearly a discrete random variable. However if we define ***Y*** as a random variable accounting for amount of rain in Seattle today. This is not countable, hence ***Y*** is not a discrete random variable, it is in fact a continous random variable which we will discuss later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After conducting many random experiments, you will notice that some outcomes are more likely than others. This is called a **Probability Distribution**. There are two important functions for probability calculations with multiple random experiments:\n",
    "\n",
    "- The Probability Mass Function (PMF)\n",
    "- The Cumulative Distribution Function (CDF)\n",
    "\n",
    "#### Probability Mass Function (PMF)\n",
    "The PMF allows you to calculate the probability of getting a particular outcome for a *discrete random variable.* For e.g. the binomial probability mass function lets you calculate probability of getting *k* heads from *n* coin flips with *p* probability of getting a heads.\n",
    "\n",
    "$$\n",
    "\\text{binomial.pmf}(k, n, p) = {n \\choose k}p^k (1-p)^{n-k}\n",
    "$$\n",
    "\n",
    "The formula basically multiplies the number of different ways you can get *k* successes out of *n* coin flips by the probability of success *p* raised to number of successes *k* and probability of failure, *1-p* raised to the number of failures *n-k*.\n",
    "\n",
    "Let's see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24609375000000025"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=5\n",
    "n=10\n",
    "p=0.5\n",
    "\n",
    "binom.pmf(k, n, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with a fair coin, we have chances of getting 5 heads for 10 throws is almost 25%. Remember here we are talking about getting exactly \"5\" heads. Let's calculate more. Probability of getting 2 heads after 10 throws with a fair coin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04394531249999999"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=2\n",
    "n=10\n",
    "p=0.5\n",
    "binom.pmf(k, n, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately 4%. Remember here we are talking about getting \"exactly\" 2 heads. Probability of getting exactly 50 heads after 100 throws with a biased coin (p=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3026227131445298e-05"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=50\n",
    "n=100\n",
    "p=0.3\n",
    "binom.pmf(k, n, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is extremely low. Which makes sense. How about probability of getting 65 heads after 100 throws with a coin biased towards the heads (p=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0467796823527298"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=65\n",
    "n=100\n",
    "p=0.7\n",
    "binom.pmf(k, n, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost 5%. As *n* gets larger, the probability of getting *k* heads smaller for the same *p*. We have been thinking about getting exactly *k* heads, how about if we think of getting *k* or **fewer** heads? Then we come to **Cumulative Distribution Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cumulative Distribution Function\n",
    "\n",
    "$$\n",
    "\\text{binomial.cdf}(k, n, p) = {n \\choose 0}p^0(1-p)^n + {n \\choose 1}p^1(1-p)^{n-1} + ... + {n \\choose k}p^k(1-p)^{n-k}\n",
    "$$\n",
    "\n",
    "CDF adds the probability of going from 0 to k heads. This basically gives us probability of getting *k* or *fewer* heads from n coin flips with probability *p* of getting heads.\n",
    "\n",
    "Basically, adding the probabilities from the mass function, we get the cumulative distribution function. This is for getting a range of probabilities rather than getting probability of a single event.\n",
    "\n",
    "So probability of getting 5 or fewer heads for 10 throws with probability of heads as 0.5, we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6230468749999999"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binom.cdf(k=5, n=10, p=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "62% of the time we will get 5 or less heads in such case. The probability of getting 50 heads or less after 100 throws with p=0.3 (biased towards tail) is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999909653138043"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binom.cdf(k=50, n=100, p=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost ~100%. Which again, makes sense. Now probability of getting 59 or **more** heads after 100 throws of a biased coin towards head (p=0.7) is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9875015928335618"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - binom.cdf(k=59, n=100, p=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost 99%. Notice the  \"1 - \" as we are calculating for more not less. We can do the same using `sf` function which stands for survival function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9875015928335618"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binom.sf(k=59, n=100, p=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Probability\n",
    "How should we update our beliefs in light of new evidence we observe?\n",
    "* **idea** :: As you obtain additional information, how should you update probabilities of events?  In other words, whenever we observe new evidence (i.e. obtain data), we acquire information that may affect our uncertainties. New information consistent with an existing belief, could make us sure of that belief. While a surprising observation could throw that belief into question.\n",
    "* In fact it is a useful perspective that all probabilities are conditional. There is always some background knowledge or assumptions built into every probability.\n",
    "* **example** :: Suppose one morning we are interested in the event __R__ that it will rain today. Let $P(R)$ represents the probability of rain before looking outside. If we look outside, we see dark clouds in the sky, then presumably our probability of rain should increase; We denote this new probability by $P(R|C)$ (read as probability of __R__ given __C__), where __C__ is the event of there being ominous clouds.  When we go from $P(R)$ to $P(R|C)$, we say that we are \"conditioning on  __C__\". As the day progresses, we obtain more and more information about the weather conditions and we can continually update our probabilities. If we observe events $C_{1}. C_{2}, C_{3}.. C_{n}$ occurred, we write the new conditional probability of rain given the evidence as $P(R|C_{1}, C_{2}..,C_{n})$. If eventually we observe that it does start raining, our conditional probability becomes 1.\n",
    "\n",
    "* **definition** :: If __A__ and __B__ are events with $P(B) > 0$, then the conditional probability of __A__ given __B__ denoted by $P(A|B)$ is defined as\n",
    "$$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$$\n",
    "    * Here __A__ is the event whose uncertainty we want to update. __B__ is the evidence we observe (or want to treat as something given). We call $P(A)$ as __prior__ probability and $P(A|B)$ as __posterior__ probability.\n",
    "    * For any event, $P(A|A) = \\frac{P(A \\cap A)}{P(A)} = 1$. Upon observing that __A__ has occurred, our updated probability of __A__ becomes 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
