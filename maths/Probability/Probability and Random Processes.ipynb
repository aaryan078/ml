{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Probability and Random Processes\n",
    " ### Random Experiment\n",
    " A random experiment is a process by which we observe something uncertain. After the experiment the result of the experiment is known.\n",
    "   #### Trial\n",
    "   If a random experiment is repeated several times, we call each of them as trial. Thus a trial is a particular performance of a random experiment. In the example of tossing a coin, each trial will result in either heads or tails.\n",
    "   #### Outcome:\n",
    "   An outcome is the result of a  random experiment.\n",
    "   \n",
    "   #### Sample Space: \n",
    "   The set of all possible outcomes of a  random experiment is called sample space.\n",
    "\t\n",
    "   #### Event\n",
    "   An event **A** is subset of sample space **S** and we way that **A** __occurred__ if the actual outcome is in  **A**\n",
    "\t\t* example:: Before tossing a coin, you won't know whether you will get heads or tails. This is a random experiment. It's sample space is {__heads, tails__} usually denoted as {__H, T__}.  Another example is if toss a coin 3 times and observe the sequence of heads/tails. The sample space here may be defined as :$$    $$S = {__(H, H, H), (H, H, T), (H, T, H), (T, H, H), (H, T, T), (T, H, T), (T, T, H), (T, T, T)__}. Let **A** can be an event that first flip is Heads.  It is indeed a subset of sample space.  A = {__(H, H, H), (H, H, T), (H, T, H)__, __(H, T, T)__}. saying that **A** occurs is the same thing as saying that the first flip is Heads.\n",
    "\t\n",
    "  ### Probability\n",
    "  A mathematical language for expressing degree of belief or uncertainties about events.\n",
    "  * *frequentist view*:: of probability is that it represents a long-run frequency over a large number of [Trial](Trial.md)s of a  random experiment. If we say a coin has 1/2 probability of heads, that means coin would land heads ~50% of time if we tossed it over and over and over.\n",
    "  * *bayesian view* :: Probability represents a degree of belief about the event in question. So that we can assign the probability to hypotheses like \"candidate A will win election\" or \"the defendant is not guilty\", even if it isn't possible to repeat the same election or crime over and over again.\n",
    "  \n",
    "   * *naive definition* :: The naive definition of probability of an event is to count the number of ways the event could happen and divide by the total number of possible outcomes for the experiment.\n",
    "\t\t\t$$P_{naive}(A) = \\frac{|A|}{|S|} = \\frac{\\text{number of outcomes favorable to A}}{\\text{total number of outcomes in S}} $$\n",
    "\t\t\t\n",
    "\t\t\t$$P_{naive}(A) = \\frac{4}{9} $$As there are 4 favorable outcomes to event A, and total number of outcomes in sample space is 9.\n",
    "\t\t\t\n",
    "   * *why naive* :: \n",
    "\t\t* Assumed that sample space is finite, and each outcomeis equally likely.\n",
    "\t\t* For example, Imagine two arguments: People think that either there is life on mars or there isn't. So they apply a 50:50 ratio without reasoning and make a conclusion that probability of life on mars is 1/2.  But by same logic probability of __intelligent__ life on mars is also 1/2. However, it is intuitively clear that latter should have strictly lower probability than former.\n",
    "\t* *where naive definition is applicable* :: \n",
    "        * Where there is __symmetry__ in problem that makes it equally likely. For example it is common to assume that coin is fair and there is equally likely to get a heads or tail due to physical symmetry of the coin. Unless mentioned, there is no doubt that a dice is fair and probability of getting any number from 1 to 6 is 1/6.\n",
    "\t\t* When the outcomes are equally likely by design. For example while conducting a survey of __n__ people in a population of __N__. A common goal is to obtain a simple random sample, which means that the n people are chosen randomly with all subsets of size n being equally likely.\n",
    "\t\t* When the naive definition serves a __null model__. In this case, we assume that naive definition applies and see what prediction we get and then compare them with real world observed data to asses if the hypothesis of equally likely outcomes is tenable.\n",
    "       \n",
    "### Conditional Probability\n",
    "How should we update our beliefs in light of new evidence we observe?\n",
    "* **idea** :: As you obtain additional information, how should you update probabilities of events?  In other words, whenever we observe new evidence (i.e. obtain data), we acquire information that may affect our uncertainties. New information consistent with an existing belief, could make us sure of that belief. While a surprising observation could throw that belief into question.\n",
    "* In fact it is a useful perspective that all probabilities are conditional. There is always some background knowledge or assumptions built into every probability.\n",
    "* **example** :: Suppose one morning we are interested in the event __R__ that it will rain today. Let $P(R)$ represents the probability of rain before looking outside. If we look outside, we see dark clouds in the sky, then presumably our probability of rain should increase; We denote this new probability by $P(R|C)$ (read as probability of __R__ given __C__), where __C__ is the event of there being ominous clouds.  When we go from $P(R)$ to $P(R|C)$, we say that we are \"conditioning on  __C__\". As the day progresses, we obtain more and more information about the weather conditions and we can continually update our probabilities. If we observe events $C_{1}. C_{2}, C_{3}.. C_{n}$ occurred, we write the new conditional probability of rain given the evidence as $P(R|C_{1}, C_{2}..,C_{n})$. If eventually we observe that it does start raining, our conditional probability becomes 1.\n",
    "\n",
    "* **definition** :: If __A__ and __B__ are events with $P(B) > 0$, then the conditional probability of __A__ given __B__ denoted by $P(A|B)$ is defined as\n",
    "$$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$$\n",
    "    * Here __A__ is the event whose uncertainty we want to update. __B__ is the evidence we observe (or want to treat as something given). We call $P(A)$ as __prior__ probability and $P(A|B)$ as __posterior__ probability.\n",
    "    * For any event, $P(A|A) = \\frac{P(A \\cap A)}{P(A)} = 1$. Upon observing that __A__ has occurred, our updated probability of __A__ becomes 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
